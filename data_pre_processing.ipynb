{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to check and download necessary NLTK resources\n",
    "def download_nltk_resources():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    try:\n",
    "        nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "    except LookupError:\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "# Call the function to ensure resources are downloaded\n",
    "download_nltk_resources()\n",
    "\n",
    "PATH = os.getcwd()\n",
    "PATH = PATH + '/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(file):\n",
    "    file_path = os.path.join(PATH,file)\n",
    "    file = pd.read_csv(file_path)\n",
    "    return file\n",
    "\n",
    "# Clean the foreign keys \n",
    "def clean_str(df,cols):\n",
    "    if isinstance(cols,list):\n",
    "        for col in cols:\n",
    "            df[col] = df[col].map(lambda x: x.strip().upper() if isinstance(x, str) else str(x))\n",
    "    elif isinstance(cols,str):\n",
    "        df[col] = df[col].map(lambda x: x.strip().upper() if isinstance(x, str) else x)\n",
    "    else:\n",
    "        raise KeyError\n",
    "\n",
    "\n",
    "def create_dataframe(path1= 'categories.csv', path2 = 'brand_category.csv', path3 = 'offer_retailer.csv'):\n",
    "\n",
    "    product_cat=load_data(path1)\n",
    "    product_cat = product_cat.drop('CATEGORY_ID',axis=1)\n",
    "\n",
    "    brand_cat  = load_data(path2)\n",
    "    brand_cat = brand_cat.drop('RECEIPTS',axis=1)\n",
    "\n",
    "    retailers = load_data(path3)\n",
    "\n",
    "    return [product_cat, brand_cat, retailers]\n",
    "\n",
    "\n",
    "# set(stopwords.words('english'))\n",
    "# remove brand, retailer and stop word form the offer string and clean things up\n",
    "def clean_string(text):\n",
    "    # Replace any character that is not a letter or a space with nothing\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def text_prep(txt,stop_words):\n",
    "    txt = clean_string(txt)\n",
    "    #if pd.notna(brand) or pd.notna(retailer):\n",
    "        #brand_retailer = str(brand).replace(\"'\",\"\").title().split()+str(retailer).replace(\"'\",\"\").title().split()\n",
    "        #for b in brand_retailer :\n",
    "        #    txt = txt.title().replace(b,\"\")\n",
    "    words = word_tokenize(txt)\n",
    "    # words =[word.strip(u\"\\u2122\").strip(u'\\u0256') for word in words]\n",
    "    filtered_text = [word for word in words if (not word.lower() in stop_words) and (word.isalpha()) \n",
    "                     ]\n",
    "    tagged = pos_tag(filtered_text)\n",
    "    nouns = [word for word, pos in tagged if pos in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "    return ' '.join(nouns)\n",
    "\n",
    "\n",
    "def reshape_to_2d(series):\n",
    "    # Convert the series to a NumPy array\n",
    "    array_1d = np.array(series)\n",
    "    \n",
    "    # Reshape to a 2D array (1 row and N columns)\n",
    "    array_2d = array_1d.reshape(1, -1)\n",
    "    \n",
    "    return array_2d\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    # Initialize the tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize the input text and prepare it as input for the model\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "\n",
    "    # Aggregate the embeddings - here we take the mean across all tokens\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Convert to one-dimensional array and return\n",
    "    return embeddings.squeeze().numpy()\n",
    "\n",
    "\n",
    "def input_prep_with_embedding(txt,stop_words):\n",
    "    txt =  clean_string(txt)\n",
    "\n",
    "    words = word_tokenize(txt)\n",
    "    # words =[word.strip(u\"\\u2122\").strip(u'\\u0256') for word in words]\n",
    "    filtered_text = [word for word in words if (not word.lower() in stop_words) and (word.isalpha())]\n",
    "    tagged = pos_tag(filtered_text)\n",
    "    nouns = [word for word, pos in tagged if pos in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "    txt = \" \".join(nouns)\n",
    "    \n",
    "    txt = get_embedding(txt)\n",
    "    \n",
    "    txt=reshape_to_2d(txt)\n",
    "\n",
    "    return txt\n",
    "\n",
    "\n",
    "def dup_cat_clean_bert(row, sw):\n",
    "    \n",
    "    cat = row['BRAND_BELONGS_TO_CATEGORY'].lower().replace('&',\" \").split()\n",
    "    cat = ' '.join(cat)\n",
    "    \n",
    "    embedding_cat = input_prep_with_embedding(cat,sw)\n",
    "\n",
    "    offer = row['OFFER_clean'].lower()\n",
    "\n",
    "    embedding_offer = input_prep_with_embedding(offer,sw)\n",
    "\n",
    "    bert_score = cosine_similarity(embedding_cat, embedding_offer)[0][0]\n",
    "\n",
    "    #fuzz_score1 = fuzz.partial_ratio(cat, offer)\n",
    "    # fuzz_score1 = fuzz.token_sort_ratio(cat, offer)\n",
    "    \n",
    "    return bert_score\n",
    "\n",
    "\n",
    "def join_str(row):\n",
    "    s = f\"\"\"{str(row['OFFER_clean']) if (pd.notna(row['OFFER_clean']) and row['OFFER_clean']!='nan') else \"\"} {str(row['BRAND']).replace('&',\" \").replace(\",\", \"\")} {str(row['RETAILER']) if (pd.notna(row['RETAILER']) and row['RETAILER']!='nan') else ''} {str(row['PRODUCT_CATEGORY']) if (pd.notna(row['PRODUCT_CATEGORY']) and row['PRODUCT_CATEGORY']!='nan') else ''} {row['IS_CHILD_CATEGORY_TO_compli'] if (pd.notna(row['IS_CHILD_CATEGORY_TO_compli']) and row['IS_CHILD_CATEGORY_TO_compli']!='nan') else ''}\"\"\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_cat(product_cat):\n",
    "    clean_str(product_cat, ['PRODUCT_CATEGORY', 'IS_CHILD_CATEGORY_TO'])\n",
    "    # attach parent's parent category to some of the category\n",
    "    product_cat_refined = product_cat.merge(product_cat[['PRODUCT_CATEGORY','IS_CHILD_CATEGORY_TO']], left_on='IS_CHILD_CATEGORY_TO',right_on='PRODUCT_CATEGORY' ,how='left',suffixes=('','_compli'))\n",
    "    product_cat_refined = product_cat_refined.drop(\"PRODUCT_CATEGORY_compli\",axis=1)\n",
    "\n",
    "    return product_cat_refined\n",
    "\n",
    "\n",
    "def seperate_offers(product_cat, brand_cat, retailers):\n",
    "    clean_str(product_cat, ['PRODUCT_CATEGORY', 'IS_CHILD_CATEGORY_TO'])\n",
    "    clean_str(brand_cat, ['BRAND','BRAND_BELONGS_TO_CATEGORY'])\n",
    "    clean_str(retailers,['RETAILER','BRAND'])\n",
    "\n",
    "    # seperate brand with unique category from brand with multiple category\n",
    "    cat_check = brand_cat.groupby(\"BRAND\")[\"BRAND_BELONGS_TO_CATEGORY\"].count()\n",
    "    unique_cat  = cat_check[cat_check<2]\n",
    "    dup_cat = cat_check[cat_check>=2]\n",
    "    unique_brand_cat = brand_cat[brand_cat['BRAND'].isin(unique_cat.index)]\n",
    "    dup_brand_cat = brand_cat[brand_cat['BRAND'].isin(dup_cat.index)]\n",
    "\n",
    "    combined_unique_brand = retailers.merge(unique_brand_cat, how='inner', on='BRAND')\n",
    "    combined_dup_brand  = retailers.merge(dup_brand_cat, how='inner', on='BRAND')\n",
    "\n",
    "    #join back those brand not exist in brand_cat\n",
    "    inner_brand = np.array(list(combined_dup_brand['BRAND'].values)+list(combined_unique_brand['BRAND'].values))\n",
    "    not_coexist_offer = retailers[retailers['BRAND'].apply(lambda row: row not in inner_brand)][:]\n",
    "\n",
    "    return [combined_unique_brand, combined_dup_brand, not_coexist_offer]\n",
    "\n",
    "def generate_stopword():\n",
    "    sw = set(stopwords.words('english'))\n",
    "    sw.update({\"buy\", \"spend\", \"select\", 'varieties', 'sizes', 'ounce', 'count', 'liter'})\n",
    "    sw.remove('any')\n",
    "    return sw\n",
    "\n",
    "def create_clean_offer(combined_unique_brand, combined_dup_brand, not_coexist_offer, sw):\n",
    "    #brand_set = set(brand_cat['BRAND'].str.capitalize())\n",
    "    combined_dup_brand['OFFER_clean'] = combined_dup_brand.apply(lambda row: text_prep(row['OFFER'],sw),axis=1)\n",
    "    combined_unique_brand['OFFER_clean'] = combined_unique_brand.apply(lambda row: text_prep(row['OFFER'],sw),axis=1)\n",
    "    not_coexist_offer['OFFER_clean'] = not_coexist_offer.apply(lambda row: text_prep(row['OFFER'],sw),axis=1)\n",
    "\n",
    "    general_offer = combined_dup_brand[combined_dup_brand['OFFER_clean'].str.lower().str.contains(\"reward|club|member\")]\n",
    "    unique_brand_offer = pd.concat([combined_unique_brand,general_offer],axis=0)\n",
    "    unique_brand_offer = pd.concat([unique_brand_offer,not_coexist_offer],axis=0)\n",
    "\n",
    "    return [unique_brand_offer, combined_dup_brand]\n",
    "\n",
    "\n",
    "def generate_training_str(unique_brand_offer, combined_dup_brand, product_cat_refined, sw):\n",
    "    mislabeled_offer = combined_dup_brand[~combined_dup_brand['OFFER_clean'].str.lower().str.contains(\"reward|club|member\")].copy()\n",
    "    mislabeled_offer['bert_score'] = mislabeled_offer.apply(lambda row: dup_cat_clean_bert(row, sw), axis=1)\n",
    "    final_dup = mislabeled_offer.groupby(['OFFER', 'BRAND']).apply(lambda x: x.loc[x['bert_score'].idxmax()]).drop('bert_score',axis=1)\n",
    "    final_dup = final_dup.reset_index(drop=True)\n",
    "\n",
    "    training  = pd.concat([unique_brand_offer,final_dup],axis=0)\n",
    "    training = training.merge(product_cat_refined, how='left' , left_on=\"BRAND_BELONGS_TO_CATEGORY\", right_on='PRODUCT_CATEGORY')\n",
    "    training['training_str'] = training.apply(join_str,axis=1).to_list()\n",
    "\n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_data(path_to_cat = 'categories.csv', path_to_brand = 'brand_category.csv', path_to_offer = 'offer_retailer.csv'):\n",
    "    product_cat, brand_cat, retailers = create_dataframe(path_to_cat, path_to_brand, path_to_offer)\n",
    "    sw = generate_stopword()\n",
    "    product_cat_refined = refine_cat(product_cat)\n",
    "    combined_unique_brand, combined_dup_brand, not_coexist_offer = seperate_offers(product_cat, brand_cat, retailers)\n",
    "    unique_brand_offer, combined_dup_brand = create_clean_offer(combined_unique_brand, combined_dup_brand, not_coexist_offer, sw)\n",
    "    training = generate_training_str(unique_brand_offer, combined_dup_brand, product_cat_refined, sw)\n",
    "    emb = training['training_str'].apply(get_embedding)\n",
    "    training['training_str_vector'] = emb\n",
    "    training.to_pickle(\"data/processed_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
