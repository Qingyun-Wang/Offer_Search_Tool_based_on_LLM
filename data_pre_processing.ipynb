{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "# Function to check and download necessary NLTK resources\n",
    "def download_nltk_resources():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "    except LookupError:\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "# Call the function to ensure resources are downloaded\n",
    "download_nltk_resources()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "PATH = PATH + '/data'\n",
    "\n",
    "def load_data(file):\n",
    "    file_path = os.path.join(PATH,file)\n",
    "    file = pd.read_csv(file_path)\n",
    "    return file\n",
    "product_cat=load_data('categories.csv')\n",
    "product_cat = product_cat.drop('CATEGORY_ID',axis=1)\n",
    "\n",
    "brand_cat  = load_data('brand_category.csv')\n",
    "brand_cat = brand_cat.drop('RECEIPTS',axis=1)\n",
    "\n",
    "retailers = load_data('offer_retailer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the foreign keys \n",
    "def clean_str(df,cols):\n",
    "    if isinstance(cols,list):\n",
    "        for col in cols:\n",
    "            df[col] = df[col].map(lambda x: x.strip().upper() if isinstance(x, str) else x)\n",
    "    elif isinstance(cols,str):\n",
    "        df[col] = df[col].map(lambda x: x.strip().upper() if isinstance(x, str) else x)\n",
    "    else:\n",
    "        raise KeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_str(product_cat, ['PRODUCT_CATEGORY', 'IS_CHILD_CATEGORY_TO'])\n",
    "clean_str(brand_cat, ['BRAND','BRAND_BELONGS_TO_CATEGORY'])\n",
    "clean_str(retailers,['RETAILER','BRAND'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_cat_refined = product_cat.merge(product_cat[['PRODUCT_CATEGORY','IS_CHILD_CATEGORY_TO']], left_on='IS_CHILD_CATEGORY_TO',right_on='PRODUCT_CATEGORY' ,how='left',suffixes=('','_compli'))\n",
    "product_cat_refined = product_cat_refined.drop(\"PRODUCT_CATEGORY_compli\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PRODUCT_CATEGORY                0\n",
       "IS_CHILD_CATEGORY_TO            0\n",
       "IS_CHILD_CATEGORY_TO_compli    83\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_cat_refined.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PRODUCT_CATEGORY\n",
       "RED PASTA SAUCE            1\n",
       "FEMININE HYGEINE           1\n",
       "LEAFY SALADS               1\n",
       "CREAM                      1\n",
       "COFFEE                     1\n",
       "                          ..\n",
       "FROZEN BEEF                1\n",
       "FROZEN SEAFOOD             1\n",
       "BATH & BODY                1\n",
       "FROZEN PLANT-BASED MEAT    1\n",
       "GUM                        1\n",
       "Name: count, Length: 118, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_cat_refined['PRODUCT_CATEGORY'].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add cat to Offer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_check = brand_cat.groupby(\"BRAND\")[\"BRAND_BELONGS_TO_CATEGORY\"].count()\n",
    "unique_cat  = cat_check[cat_check<2]\n",
    "dup_cat = cat_check[cat_check>=2]\n",
    "unique_brand_cat = brand_cat[brand_cat['BRAND'].isin(unique_cat.index)]\n",
    "unique_brand_cat\n",
    "\n",
    "dup_brand_cat = brand_cat[brand_cat['BRAND'].isin(dup_cat.index)]\n",
    "dup_brand_cat\n",
    "\n",
    "combined_unique_brand = retailers.merge(unique_brand_cat, how='inner', on='BRAND')\n",
    "combined_dup_brand  = retailers.merge(dup_brand_cat, how='inner', on='BRAND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_offer_info(x):\n",
    "#     offer_info= set(str(x['OFFER_clean']).lower().split(\" \"))\n",
    "#     cat_info = set(str(x['BRAND_BELONGS_TO_CATEGORY']).lower().split(\" \"))\n",
    "\n",
    "#     if str(x['BRAND_BELONGS_TO_CATEGORY']).lower() in str(x['OFFER']).lower():\n",
    "#         return x['BRAND_BELONGS_TO_CATEGORY']\n",
    "#     elif offer_info & (cat_info):\n",
    "#         return x['BRAND_BELONGS_TO_CATEGORY']\n",
    "#     else:\n",
    "#         return np.nan\n",
    "\n",
    "# combined_unique_brand['refined_cat'] = combined_unique_brand.apply(extract_offer_info,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean OFFER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set(stopwords.words('english'))\n",
    "def text_prep(txt,brand,retailer,stop_words):\n",
    "    txt = re.sub(r'\\$?\\d+', '', txt)\n",
    "    txt = txt.replace(\"'\",\"\").replace(\"$\",\"\").replace('-',\" \")\n",
    "\n",
    "    if pd.notna(brand) or pd.notna(retailer):\n",
    "        brand_retailer = str(brand).replace(\"'\",\"\").title().split()+str(retailer).replace(\"'\",\"\").title().split()\n",
    "        for b in brand_retailer :\n",
    "            txt = txt.title().replace(b,\"\")\n",
    "    words = word_tokenize(txt)\n",
    "    # words =[word.strip(u\"\\u2122\").strip(u'\\u0256') for word in words]\n",
    "    filtered_text = [word for word in words if (not word.lower() in stop_words) and (word.isalpha()) \n",
    "                     ]\n",
    "    # tagged = pos_tag(filtered_text)\n",
    "    # nouns = [word for word, pos in tagged if pos in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dup_cat_clean(row):\n",
    "    cat = row['BRAND_BELONGS_TO_CATEGORY'].lower().replace('&',\" \").split()\n",
    "    cat = ' '.join(cat)\n",
    "    offer = row['OFFER_clean'].lower()\n",
    "    fuzz_score1 = fuzz.partial_ratio(cat, offer)\n",
    "    # fuzz_score1 = fuzz.token_sort_ratio(cat, offer)\n",
    "    \n",
    "    return fuzz_score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "sw.update({\"buy\", \"spend\", \"select\", 'varieties', 'sizes', 'ounce', 'count', 'liter'})\n",
    "sw.remove('any')\n",
    "brand_set = set(brand_cat['BRAND'].str.capitalize())\n",
    "combined_dup_brand['OFFER_clean'] = combined_dup_brand.apply(lambda row: text_prep(row['OFFER'],row['BRAND'],row['RETAILER'],sw),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### add general offer to unique dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_offer = combined_dup_brand[combined_dup_brand['OFFER_clean'].str.lower().str.contains(\"reward|club|member\")]\n",
    "unique_brand = pd.concat([combined_unique_brand,general_offer],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### filter duplicate offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabeled_offer = combined_dup_brand[~combined_dup_brand['OFFER_clean'].str.lower().str.contains(\"reward|club|member\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabeled_offer['CAT_score'] = mislabeled_offer.apply(dup_cat_clean,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dup = mislabeled_offer.groupby(['OFFER', 'BRAND']).apply(lambda x: x.loc[x['CAT_score'].idxmax()]).drop('CAT_score',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dup = final_dup.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training  = pd.concat([unique_brand,final_dup],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training.merge(product_cat_refined, how='left' , left_on=\"BRAND_BELONGS_TO_CATEGORY\", right_on='PRODUCT_CATEGORY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_str(row):\n",
    "    s = f\"\"\"{str(row['OFFER_clean']) if pd.notna(row['OFFER_clean']) else \" \"} {str(row['BRAND']).replace('&',\" \").replace(\",\", \" \")} {str(row['RETAILER']) if pd.notna(row['RETAILER']) else \" \"} {row['PRODUCT_CATEGORY']}, {row['IS_CHILD_CATEGORY_TO_compli'] if pd.notna(row['IS_CHILD_CATEGORY_TO_compli']) else row['PRODUCT_CATEGORY']}\"\"\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['training_str'] = training.apply(join_str,axis=1).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.to_csv(f\"{PATH}/processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "sw.update({\"buy\", \"spend\", \"select\", 'varieties', 'sizes', 'ounce', 'count', 'liter'})\n",
    "sw.remove('any')\n",
    "\n",
    "def input_prep_fz(txt,stop_words):\n",
    "    txt = re.sub(r'\\$?\\d+', '', txt)\n",
    "    txt = txt.replace(\"'\",\"\").replace(\"$\",\"\").replace('-',\" \")\n",
    "\n",
    "    words = word_tokenize(txt)\n",
    "    # words =[word.strip(u\"\\u2122\").strip(u'\\u0256') for word in words]\n",
    "    filtered_text = [word for word in words if (not word.lower() in stop_words) and (word.isalpha())]\n",
    "    tagged = pos_tag(filtered_text)\n",
    "    nouns = [word for word, pos in tagged if pos in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "    return \" \".join(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'milk nauns'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prep_fz(\"I want some milk, this is a nauns. ok, I want\",sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
